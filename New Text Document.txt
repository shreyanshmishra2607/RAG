                   ┌────────────────────┐
                   │  User Input Query  │
                   └────────┬───────────┘
                            │
               ┌────────────▼────────────┐
               │  Query Intent Classifier │ ← (LLM or rules)
               └───────┬────────┬────────┘
                       │        │
        ┌──────────────┘        └──────────────┐
        ▼                                     ▼
┌──────────────┐                       ┌──────────────┐
│ Needs Real-  │                       │ General Query│
│ Time Info    │                       │ (Static info)│
└──────┬───────┘                       └──────┬───────┘
       │                                    │
       ▼                                    ▼
┌──────────────┐                      ┌──────────────┐
│ Web Search   │                      │ Vector Search│
│ (e.g. Searx) │                      │ (ChromaDB)   │
└──────┬───────┘                      └──────┬───────┘
       ▼                                    ▼
┌──────────────┐                      ┌──────────────┐
│ Chunk & Embed│                      │ Embed Query  │
│ (Arctic/Nomic)                     │ Search Similar│
└──────┬───────┘                      └──────┬───────┘
       ▼                                    ▼
       └────────────┬────────────┬──────────┘
                    ▼            ▼
                ┌────────────────────┐
                │  Context Assembler │ ← (Top chunks from either)
                └────────┬───────────┘
                         ▼
                ┌────────────────────┐
                │   LLM Generation   │ ← (GPT-4o, Claude, Mixtral)
                └────────┬───────────┘
                         ▼
                ┌────────────────────┐
                │   Final Response   │
                └────────────────────┘


🧠 Modular Backend Plan (Only)
✅ 1. Input Handler
Accept user query via FastAPI / Flask

Preprocess the text (strip, lowercase, etc.)

✅ 2. Intent Classifier
Rule-based (use keywords for now)

Later: Upgrade to LLM-based classifier (Claude/GPT)

✅ 3. Routing Logic
If real-time → go to Web Search

Else → go to Chroma Vector DB

✅ 4. Chunk & Embed (Dynamic or Preprocessed)
Use nomic-embed-text-v1.5 or snowflake-arctic-embed

Chunk using langchain or manual recursive splitting

For real-time: chunk search results on the fly

For internal: already embedded and stored in Chroma

✅ 5. Vector Search (ChromaDB)
Query embedding → similarity search

Return top-k chunks

✅ 6. Context Assembler
Format top-k chunks into a final structured prompt

Add user query + instructions

✅ 7. LLM Caller
Use GPT-4o, Claude 3, or open-source Mixtral

Pass prompt + return generated answer

✅ 8. Response Formatter
Clean final output

Add optional metadata (e.g., citations, source titles)

✅ 9. Logs / Memory (Optional for Now)
Log user queries, responses, embeddings in JSON or DB

🔧 Suggested Tech Stack (Backend Only)
Component	Tool/Library
API Framework	FastAPI (lightweight + async)
Embedding Model	nomic-embed-text or arctic-embed
Vector DB	ChromaDB (easy + local)
Chunking	LangChain RecursiveTextSplitter
Web Search	SearxNG API or SerpAPI
LLM API	OpenAI (gpt-4o) / Anthropic / Mixtral
Async Tasks	asyncio, httpx, or celery
Logging	JSON logs / SQLite / PostgreSQL

💡 Bonus Ideas for Later
✅ Add caching with Redis for repeated queries

✅ Add document ingestion (PDF, DOCX) to pre-embed

✅ Add agent tools (math, code exec, etc.)